---
title: "Introduction about Hierarchical Clustering"
author: "Nabiilah Ardini Fauziyyah dan Inayatus Sholikhah"
date: "`r format(Sys.Date(), '%e %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_float: 
      collapsed: true
    df_print: paged
    theme: united
    highlight: zenburn
---

<style>

body {
text-align: justify}

</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      fig.align = "center")
options(scipen = 99)
library(dplyr)
```

# Clustering

Clustering merupakan salah satu metode yang termasuk kedalam unsupervised learning. Clustering bertujuan untuk melakukan pengelompokan pada suatu set data yang memiliki kemiripan berdasarkan jarak terdekatnya. Clustering memiliki karakteristik dimana anggota dalam satu cluster memiliki kemiripan yang sama atau jarak yang sangat dekat, namun anggota antar cluster memiliki kemiripan yang sangat berbeda atau jarak yang sangat jauh. Menurut (Tan et al., 2006) dalam bukunya yang berjudul *Introduction to Data Mining*, metode clustering dibagi menjadi dua jenis, yaitu Hierarchical Clustering dan Partitional Clustering[^1]. 

**Partitional Clustering** umumnya bertujuan untuk mengelompokkan data menjadi beberapa cluster yang lebih kecil[^2]. Pada prosesnya, setiap cluster akan memiliki titik pusat cluster (centroid) dan mencoba menghitung setiap data yang paling dekat dengan centroid tersebut. Metode dalam partitional clustering diantaranya k-means, fuzzy k-means, dan mixture modelling.

```{r echo=FALSE}
knitr::include_graphics("image/partitional.png")
```

Sedangkan dalam **Hierarchical Clustering**, pengelompokan data dilakukan dengan membuat suatu bagan hirarki (**dendrogram**) dengan tujuan menunjukkan kemiripan antar data[^2]. Setiap data yang mirip akan memiliki hubungan hirarkis yang dekat, dan dendrogram terus terbentuk hingga dihasilkan satu kelompok besar. Cluster dapat dihasilkan dengan memotong struktur hirarkis pada level tertentu. Beberapa metode dalam hierarchical clustering yaitu single linkage, complete linkage, average linkage, dan ward's minimum variance.

```{r echo=FALSE}
knitr::include_graphics("image/hc.png")
```
Pada kesempatan kali ini kita akan mendalami terkait Hierarchical Clustering serta aplikasinya untuk pengolahan data.

# Hierarchical Clustering

Secara umum, hierarchical clustering dibagi menjadi dua jenis yaitu *agglomerative* dan *divisive*[^3]. Kedua metode ini dibedakan berdasarkan cara dalam melakukan pengelompokan data hingga membentuk bagan hirarki (dendrogram), menggunakan bottom-up atau top-down manner.

1. **Agglomerative clustering** 

Agglomerative clustering biasa disebut juga sebagai agglomerative nesting (AGNES) dimana cara kerja dalam melakukan pengelompokan hirarki menggunakan **bottom-up manner**. Prosesnya dimulai dengan menganggap setiap data sebagai 1 cluster kecil (leaf) yang hanya memiliki 1 anggota saja, lalu pada tahap selanjutnya dua cluster yang memiliki kemiripan akan dikelompokkan menjadi 1 cluster yang lebih besar (nodes). Proses ini akan dilakukan terus menerus hingga semua data menjadi satu cluster besar (root). 

2. **Divisive hierarchical clustering**

Divisive hierarchical clustering biasa disebut juga sebagai divisive analysis (DIANA) dimana cara kerja dalam mengelompokkan data menggunakan **top-down manner**. Prosesnya dimulai dengan menganggap satu set data sebagai satu cluster besar (root), lalu dalam setiap iterasinya setiap data yang memiliki karakteristik yang berbeda akan dipecah menjadi 2 cluster yang lebih kecil (nodes) dan proses akan terus berjalan hingga setiap data menjadi 1 cluster kecil (leaf) yang hanya memiliki 1 anggota saja.

Berikut ini perbedaan cara kerja agglomerative dan divisive clustering bekerja.

```{r echo=FALSE}
knitr::include_graphics("image/agnes-vs-diana.png")
```

Selain memahami proses pembuatan dendrogramnya, mari coba memahami bagaimana node-node (tiap cluster) terbuat dan digabungkan.

Tujuan dari clustering secara umum, baik hierarchical maupun partitional clustering adalah untuk membuat cluster yang memiliki karakteristik yang sama dalam satu anggota cluster dan memiliki karakteristik yang berbeda antar clusternya. Konsep inilah yang mengharuskan proses pembuatan cluster untuk memperhatikan **(dis)similarity** / ukuran ketidakmiripan antar clusternya. 

Tingkat (dis)similarity antar anggota cluster dapat direpresentasikan dengan **jarak** (atau beberapa menyebutnya **distance matrix**). Terdapat beragam pilihan distance matrix yang pemakaiannya bergantung pada tipe data/topik analisis yang sedang digunakan (euclidean distance, manhattan, dst)[^3].

## (Dis)similarity Measure

Hal yang penting dilakukan pertama kali saat ingin melakukan analisis clustering yaitu melakukan perhitungan (dis)similarity. Pemilihan metode (dis)similarity akan menentukan seberapa mirip suatu data untuk dijadikan kedalam satu cluster. Dalam metode clustering AGNES, (dis)similarity ini digunakan untuk membentuk distance matrix. Pengukuran (dis)similarity yang biasa digunakan adalah *euclidean distance* dan *manhattan distance*, namun bisa saja menggunakan pengukuran jarak yang lain, bergantung pada data yang sedang kita analisis. Berikut ini formula dalam perhitungan (dis)similarity dari kedua metode tersebut:

1. *Euclidean distance*

$$d_{xy} = \sqrt {\sum_{i=1}^{n}(x_i - y_i)^2}$$

2. *Manhattan distance*

$$d_{xy} = \sum_{i=1}^{n} |{(x_i - y_i)}|$$

Ada beberapa pengukuran (dis)similarity yang lain yang bisa digunakan yaitu menggunakan *correlation-based distance*. Correlation-based distance biasa digunakan ketika kita ingin mengetahui bentuk (dis)similarity pada suatu data yang bergerak "naik" atau "turun" secara bersamaan. Pengukuran (dis)similarity ini sering digunakan untuk melakukan analisis ekspresi gen atau dalam dunia marketing, ketika kita ingin melakukan customer segmentation berdasarkan kesamaan barang yang dibeli oleh pelanggan tanpa memperhatikan banyak barang yang mereka beli. 

Euclidean distance dan manhattan distance cenderung memiliki konsep yang berkebalikan dengan correlation-based distance, data yang akan dikelompokkan bersama merupakan data yang memiliki karakteristik nilai yang sama, entah sama besarnya atau sama kecilnya. Pengukuran ini biasa digunakan pada kasus customer segmentation yang memperhatikan banyaknya pembelian dari pelanggan, segmentasi daerah yang memiliki kasus COVID tinggi/rendah, dan lain sebagainya[^5]. Pada R, untuk menghitung (dis)similarity bisa menggunakan fungsi `dist()`. Secara default, fungsi `dist()` akan menghitung euclidean distance antar observasi. 

Dalam hierarchical clustering, selain menghitung (dis)similarity antar observasi, diperlukan juga cara untuk menghitung (dis)similarity antar 2 cluster observasi sehingga dapat terbentuk dendrogram dari cluster-cluster yang ada. Proses penggabungan cluster-cluster kecil menjadi satu dendrogram utuh dilakukan menggunakan **linkage method**. Berikut ini beberapa jenis linkage method yang sering digunakan:

1. **Complete Linkage** / **Maximum Linkage**
2. **Single Linkage** / **Minimum Linkage**
3. **Average Linkage**
4. **Centroid Linkage**
5. **Ward's minimum Variance**

## Complete/Maximum Linkage

Pengukuran (dis)similarity atau jarak antar cluster dilakukan dengan mengukur terlebih dahulu jarak antar tiap observasi dari cluster yang berbeda (**pairwise distances**). Kemudian, jarak paling tinggi (maximum distance) akan menjadi ukuran (dis)similarity antar cluster. Hal ini membuat dendrogram yang terbentuk menjadi lebih terpisah antar clusternya (terbentuk cluster yang "compact").

Berikut formula jarak antar cluster menggunakan complete linkage:

$$d_{12} = \max_{ij} d(X_i, Y_j)$$

di mana:

* $X_1, X_2, ..., X_k$ : observasi pada cluster 1
* $Y_1, Y_2, ..., Y_k$ : observasi pada cluster 2
* $d(X, Y)$ : jarak antara data pada cluster 1 dengan data pada cluster 2

## Single/Minimum Linkage

Pengukuran (dis)similarity atau jarak antar cluster dilakukan dengan mengukur terlebih dahulu jarak antar tiap observasi dari cluster yang berbeda pairwise distances. Kemudian, jarak paling kecil (minimum distance) akan menjadi ukuran (dis)similarity antar cluster. Hal ini membuat dendrogram yang terbentuk menjadi lebih "loose" atau berdekatan antar clusternya.

Berikut formula jarak antar cluster menggunakan single linkage:

$$d_{12} = \min_{ij} d(X_i, Y_j)$$


## Average Linkage

Pengukuran (dis)similarity atau jarak antar cluster dilakukan dengan mengukur terlebih dahulu jarak antar tiap observasi dari cluster yang berbeda pairwise distances. Kemudian, dihitung rata-rata jarak dari pairwise distance tersebut dan nilai tersebut akan menjadi ukuran (dis)similarity antar cluster. Umumnya akan menghasilkan rupa cluster antara "loose" dan "compact".

Berikut formula jarak antar cluster menggunakan average linkage:

$$d_{12} = \frac{1}{kl} \sum_{i=1}^{k}\sum_{j=1}^{l} d(X_i, Y_j)$$

## Centroid Linkage

Perhitungan (dis)similarity atau jarak antar cluster dilakukan dengan mengukur jarak antar centroid pada dua cluster. Perhitungan centroid disini menggunakan rata-rata pada suatu variabel x. Dendrogram yang akan terbentuk adalah berdasarkan cluster yang memiliki jarak centroid paling kecil.

Berikut formula jarak antar cluster menggunakan centroid linkage:

$$d_{12} = d(\bar X, \bar Y)$$

## Ward's Minimum Variance

Perhitungan (dis)similarity atau jarak antar cluster dengan meminimalkan nilai variansi total within-cluster. Setiap cluster yang memiliki jarak minimum antar clusternya akan digabungkan menjadi satu menjadi sebuah dendrogram.

Berikut adalah ilustrasi untuk kelima jenis linkage di atas[^4]:

```{r echo=FALSE, fig.width="150%"}
knitr::include_graphics("image/linkage.png")
```

# Hierarchical Clustering Application

Keunggulan dari hierarchical clustering yang tidak dimiliki oleh metode clustering lain adalah dapat membuat dendrogram yang merepresentasikan kedekatan antar data. Hal ini amat bermanfaat khususnya dalam analisis network/komunitas atau deteksi data dari suatu komunitas. Oleh karena itu hierarchical clustering banyak dilakukan pada kasus-kasus berikut:

1. [Social Network Community Detection](https://www.hindawi.com/journals/complexity/2017/3719428/)
2. [Analisis Evolusi Mahluk Hidup - Biodiversity of Butterflies](https://www.floridamuseum.ufl.edu/science/at-last-butterflies-get-a-bigger-better-evolutionary-tree/)
3. [Analisis Penyebaran Penyakit - Nextrain for COVID19 Tracking](https://academic.oup.com/bioinformatics/article/34/23/4121/5001388)

# Additional Notes

## What to Tune?

Kasus yang berbeda akan menggunakan beberapa setting yang berbeda dalam pembuatan hierarchical clustering. Berikut adalah hal-hal yang dapat dipertimbangkan dalam pembuatan hierarchical clustering:

* Distance Matrix apa yang sebaiknya digunakan?

Hal ini amat bergantung pada data yang kita olah. Bacaan lebih lanjut tentang beragam tipe distance matrix untuk tiap tipe data dapat dilihat [disini](https://people.revoledu.com/kardi/tutorial/Similarity/index.html).

* Linkage Method apa yang sebaiknya digunakan?

Linkage method akan menentukan rupa dendrogram yang terbentuk. Telah dilakukan diskusi terkait beragam linkage method dan pemilihannya [disini](https://stats.stackexchange.com/questions/195446/choosing-the-right-linkage-method-for-hierarchical-clustering) .

## Pros & Cons

Sebelum menggunakan hierarchical clustering, ada baiknya untuk mempertimbangkan beberapa kelebihan dan kekurangannya.

Kelebihan:

* Mampu menggambarkan kedekatan antar data dengan dendrogram.
* Cukup mudah untuk pembuatannya.
* Dapat menentukan banyak cluster yang terbentuk setelah dendrogram terbentuk.

Kekurangan:

* Tidak dapat menganalisis data kategorik secara langsung (terhambat pada penghitungan jarak yang hanya bisa dilakukan untuk data numerik, sehingga data kategorik perlu dipre-process terlebih dahulu).
* Tidak diperuntukkan untuk menghasilkan jumlah cluster optimal yang mutlak (jumlah cluster dapat berubah-ubah tergantung level pemotongan dendrogram).
* Sensitif terhadap data yang memiliki skala berbeda (sehingga data perlu dinormalisasi/standarisasi terlebih dahulu).
* Sensitif terhadap outlier.
* Cukup berat komputasinya untuk data berukuran besar.

# Dendrogram & Interpretation

Setelah menghasilkan dendrogram dari hierarchical clustering, sudah sepatutnya kita perlu mampu membaca dendrogram tersebut. Sebagai contoh, berikut adalah gambar kedekatan beberapa instrumen musik yang digambarkan dengan dendrogram:

```{r echo=FALSE}
knitr::include_graphics("image/dendrogram.jpg")
```

Dendrogram sejatinya merupakan struktur yang menggambarkan kemiripan antar data. Tiap data kita pada awalnya diletakan di suatu level dasar dimana tiap data berdiri sendiri sebagai satu cluster tunggal. Pada contoh di atas, tiap instrumen musik merupakan 1 cluster tunggal di bagian dasar dendrogram. 

Kemudian, 2 data yang berdekatan akan dihubungkan dengan suatu garis yang bersatu di titik tertentu sehingga menjadi cluster yang lebih besar. Contohnya cluster instrumen "piccolo" & "flute" dan cluster instrumen "basoon" & "clarinet". 

Panjang garis antar data (dihitung tegak lurus dari titik data hingga titik kedua garis bersatu) mewakilkan nilai (dis)similarity antar data (umumnya berupa nilai distance matrix yang dipakai). Disini kita bisa menyimpulkan bahwa "piccolo" dengan "flute" memiliki kedekatan yang lebih tinggi dibandingkan "basoon" dengan "clarinet". Kita juga bisa mengetahui bahwa "flute" memiliki kedekatan yang lebih tinggi dengan "clarinet" dibandingkan dengan "trumpet". Hal ini karena "flute" dan "clarinet" memiliki panjang garis antar data yang lebih pendek dibandingkan antara "flute" dan "trumpet".

Pada dendrogram, kita juga tetap dapat melakukan partisi antar data atau membuat cluster-cluster data dengan memotong dendrogram di nilai distance / (dis)similarity tertentu. Pada contoh ini kita membuat partisi berupa cluster "strings", "woodwind", "brass", dan "percussion".

Layaknya menentukan kedekatan antar data, kita juga dapat menetukan kedekatan antar cluster dengan memperhitungkan panjang garis antar data. Dari dendrogram di atas kita bisa menarik insight bahwa cluster "woodwind" lebih dekat dengan cluster "strings" dibandingkan dengan cluster "brass" dan "percussion".

Bila disimpulkan, semakin panjang garis antar data maka semakin berbeda antar data/cluster tersebut, dan semakin pendek garis antar data maka semakin mirip antar data/cluster tersebut. Dari pemahaman ini, kita bisa menentukan manakah data yang berdekatan/berjauhan dengan data yang sedang kita analisis.


# Cluster Analysis

Meskipun kita tidak diwajibkan untuk menentukan nilai `k` (jumlah cluster yang ingin dibentuk), terdapat beberapa hal yang perlu diperhatikan dari cluster-cluster yang terbentuk pada dendrogram. 

Seperti yang diketahui bahwa agglomerative clustering bekerja dengan konsep "bottom-up" yang mana artinya setiap data akan menjadi cluster-cluster dan setiap cluster kecil yang memiliki kesamaan akan digabungkan menjadi satu cluster hingga seluruh data menjadi satu cluster besar. Dalam analisis cluster, akan sangat mungkin kita memperoleh cluster yang hanya memiliki satu atau sedikit anggota saja sehingga menyebabkan banyaknya cluster yang terbentuk. Ketika hal tersebut terjadi, kita perlu melakukan pengecekan kembali pada data yang kita miliki. Hal ini bisa disebabkan karena adanya data yang cukup berbeda dengan yang lainnya atau bisa disebut sebagai outlier / anomali. Data yang seperti ini akan menyebabkan adanya cluster yang memiliki satu anggota.

Berikut ini adalah langkah yang dilakukan untuk melakukan cluster analisis:

1. Menyiapkan data dimana data yang digunakan adalah data bertipe numerik
2. Menghitung (dis)similarity atau jarak antar data yang berpasangan pada dataset untuk digunakan sebagai distance matrix
3. Membuat cluster dendrogram menggunakan beberapa metode linkage
4. Menentukan dimana akan melakukan pemotongan cluster tree. Disinilah tahap dimana banyak cluster akan terbentuk.


# Study Case `USArrest`

Pada artikel ini, akan dicoba dijelaskan cara kerja metode clustering AGNES menggunakan data `USArrest`. Sebelum membuat pemodelan clustering, kita harus menyiapkan terlebih dahulu data tersebut. Selain itu kita pelu untuk membuat distance matrix karena metode AGNES akan mengolah data dalam bentuk distance matrix. 

## Data Pre-Paration

Pertama, kita load terlebih dahulu data yang akan digunakan.
```{r echo=FALSE}
head(USArrests)
```

Perlu diperhatikan bahwa, clustering analisis digunakan untuk melakukan pengolahan data numerik. Oleh karena itu, ketika terdapat variabel kategorik, maka kita bisa tidak menggunakan variabel tersebut untuk sementara. Selain itu, perhitungan (dis)similarity akan sangat terpengaruh berdasarkan ukuran dari data tersebut. Oleh karena itu, kita perlu melakukan standarisasi data agar ukurannya sama. Dalam melakukan standarisasi data, kita bisa menggunakan perhitungan *z-score standarization*.

Formula *z-score standarization* adalah sebagai berikut:

$$Z-Score = \frac{x - \mu}{\sigma}$$
 
 di mana:
 
* `x`: observasi data
* $\mu$: rata-rata variabel
* $\sigma$: standar deviasi pada variabel tersebut
 
Pada R, untuk melakukan z-score standarization terdapat fungsi `scale()`. Pada data `USArrest` setiap variabel belum memiliki range yang sama sehingga perlu dilakukan standarisasi. 

```{r}
summary(USArrests)
```

Hasil standarisasi data akan kita simpan dalam objek `us_z`.
```{r}
us_z <- scale(USArrests) 
head(us_z)
```

Selanjutnya kita perlu memastikan ada tidaknya missing data. Untuk melakukan pengecekan missing data, kita bisa menggunakan fungsi `anyNA()`.
```{r}
anyNA(us_z)
```
## Mengukur (Dis)similarity Data

Karena data yang kita miliki sudah maka tahap selanjutnya yaitu melakukan perhitungan (dis)similarity pada data. Metode distance yang akan digunakan pada kasus ini yaitu euclidean distance.

```{r}
us_dist <- dist(x = us_z, method = "euclidean")
```
```{r echo=FALSE}
m <- us_dist %>% as.matrix()
m[1:5, 1:5]
```

## Modeling {.tabset}

Setelah persiapan data selesai, maka saatnya membuat model clustering. Pembuatan model clustering yang akan dicoba yaitu 5 metode pada AGNES clustering yang telah dijelaskan pada bab sebelumnya. Hal ini bertujuan untuk membandingkan metode cluster mana yang lebih baik.

Pembuatan model AGNES clustering pada R dapat menggunakan fungsi `hclust()`. Fungsi ini akan meminta 2 argumen untuk dimasukkan yaitu:

* `d`: distance matrix
* `method`: metode clustering yang digunakan, terdapat beberapa pilihan diantaranya yaitu "ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median" atau "centroid".

* **Complete Linkage**
```{r}
us_hc_complete <- hclust(d = us_dist, method = "complete")
```

* **Single Linkage**
```{r}
us_hc_single <- hclust(d = us_dist, method = "single")
```

* **Average Linkage**
```{r}
us_hc_avg <- hclust(d = us_dist, method = "average")
```

* **Centroid Linkage**
```{r}
set.seed(100)
us_hc_centroid <- hclust(d = us_dist, method = "centroid")
```

* **Ward's Minimum Variance**
```{r}
us_hc_ward <- hclust(d = us_dist, method = "ward.D2")
```

### Dendrogram Complete Linkage

Pada setiap metode clustering tersebut akan membentuk sebuah dendrogram. Dendrogram dapat divisualisasikan menggunakan fungsi `plot()` biasa atau bisa menggunakan fungsi `fviz_dend()` dari package `factoextra`.

```{r, fig.height=5}
library(factoextra)
fviz_dend(us_hc_complete, cex = 0.5, main = "Cluster Dendrogram Complete Linkage")
```

Pada dendrogram tersebut, setiap observasi yang memiliki kemiripan akan dihubungkan oleh sebuah garis yang membentuk cabang/nodes yang baru. Setiap garis yang dihasilkan memiliki ketinggian yang berbeda-beda. Oleh karena itu pada y-axis terdapat "height" yang menjelaskan (dis)similarity atau jarak antar kedua observasi. Height yang ditunjukkan pada dendrogram tersebut disebut sebagai *cophenetic distance*.

Hasil yang terbentuk pada dendrogram complete linkage menunjukkan dendrogram yang cukup compact, di mana setiap cluster terlihat memiliki similarity yang jelas. Hal ini akan mempermudah saat pemotongan dendrogram untuk membentuk banyak cluster. 

Kembali pada tujuan awal, melakukan pengelompokan pada data. Menggunakan dendrogram, kita bisa melakukan pemotongan pada dendrogram tersebut pada ketinggian tertentu untuk mendapatkan banyak cluster yang terbentuk. 

Menggunakan dendrogram yang diperoleh, kita bisa menentukan berapa banyak cluster yang akan kita bentuk dan melakukan pemotongan dendrogramnya. Untuk melakukan pemotongan dendrogram, R menyediakan fungsi `cutree()` dengan mendefinisikan banyak cluter (`k`) yang akan kita bentuk.

Pada metode complete linkage ini akan kita potong menjadi 4 cluster.
```{r}
complete_clust <- cutree(us_hc_complete,  k = 4)
head(complete_clust)
```

Berikut ini banyak anggota dari masing-masing cluster yang terbentuk.
```{r}
table(complete_clust)
```
Anggota pada masing-masing cluster cukup merata dengan cluster yang paling banyak memiliki anggota adalah cluster 3. 

Berikut hasil akhir cluster yang terbentuk apabila dilihat dari dendrogram yang terbentuk.
```{r, fig.height=5}
fviz_dend(us_hc_complete, k = 4, k_colors = "jco", rect = T, main = "Complete Linkage Cluster")
```

Cara membaca cluster pada dendrogram sebagai berikut :

* Cluster 1 digambarkan oleh dendrogram berwarna merah.
* Cluster 2 digambarkan oleh dendrogram berwarna abu-abu.
* Cluster 3 digambarkan oleh dendrogram berwarna kuning.
* Cluster 4 digambarkan oleh dendrogram berwarna biru.

Salah satu cara yang dapat digunakan untuk memvalidasi cluster tree yaitu dengan menghitung korelasi antara cophenetic distance dengan jarak original masing-masing observasi. Semakin kuat nilai korelasi jarak antar observasi, maka cluster tersebut merepresentasikan data yang dimiliki.

Untuk melihat korelasinya, maka kita perlu menghitung cophenetic distance terlebih dahulu menggunakan fungsi `cophenetic()`.

```{r}
complete_coph <- cophenetic(us_hc_complete)
cor(complete_coph, us_dist)
```

### Dendrogram Single Linkage

```{r, fig.height=5}
fviz_dend(us_hc_single, cex = 0.5, main = "Cluster Dendrogram Single Linkage")
```

Hasil dendrogram yang terbentuk pada model single linkage terlihat lebih lebar di mana antar anggota cluster terlihat rapat dan sangat dekat. Hal ini dikarenakan proses pembuatan hirarki dendrogram menggunakan minimum (dis)similarity pada setiap clusternya. 

Sangat sulit untuk menentukan banyak cluster yang akan terbentuk apabila setiap observasi sangat rapat. Apabila dicoba untuk membentuk 4 cluster, maka berikut ini hasil cluster yang terbentuk.
```{r}
single_clust <- cutree(us_hc_single, k = 4)
head(single_clust)
```
Banyak anggota pada masing-masing cluster adalah sebagai berikut.
```{r}
table(single_clust)
```
Anggota pada cluster yang terbentuk sangat berbeda secara signifikan. Cluster 1 memiliki anggota yang paling banyak hampir sebanyak total observasi pada data. Hal ini yang dikarenakan terlalu rapatnya antar observasi sehingga sulit dipisahkan perbedaannya.
```{r, fig.height=5}
fviz_dend(us_hc_single, k = 4, k_colors = "jco", rect = T, main = "Single Linkage Cluster")
```

Cara membaca cluster pada dendrogram yaitu  sebagai berikut :

* Cluster 1 digambarkan oleh dendrogram berwarna merah.
* Cluster 2 digambarkan oleh dendrogram berwarna biru.
* Cluster 3 digambarkan oleh dendrogram berwarna abu-abu.
* Cluster 4 digambarkan oleh dendrogram berwarna kuning.

Apabila kita cek validasi dari cluster tree yang terbentuk, korelasi yang diperoleh sangat rendah, artinya cluster yang terbentuk kurang merepresentasikan kondisi data yang ada. Sehingga menggunakan single linkage untuk data `USArrest` kurang relevan.

```{r}
single_coph <- cophenetic(us_hc_single)
cor(single_coph, us_dist)
```


### Dendrogram Average Linkage

```{r, fig.height=5}
fviz_dend(us_hc_avg, cex = 0.5, main = "Cluster Dendrogram Average Linkage")
```

Hasil dendrogram yang terbentuk pada model average linkage terlihat hampir mirip seperti complete linkage, yaitu terlihat compact. Model average linkage biasa digunakan karena menghasilkan nilai validasi cluster tree yang lebih baik dibandingkan metode linkage yang lain. 

```{r}
avg_coph <- cophenetic(us_hc_avg)
cor(avg_coph, us_dist)
```

Apabila dendrogramnya dipotong untuk membentuk 4 cluster, berikut hasil yang diperoleh.
```{r}
avg_clust <- cutree(us_hc_avg, k = 4)
table(avg_clust)
```
Anggota cluster paling banyak diperoleh pada cluster 4 sebanyak 30 cluster. Namun terdapat cluster yang hanya memiliki 1 anggota saja yaitu pada cluster 2. Hal ini dapat dijadikan indikasi bahwa observasi pada cluster 2 yaitu "Alaska" merupakan anomali data. 

Berikut visualisasi dendrogram yang terbentuk pada masing-masing cluster.
```{r, fig.height=5}
fviz_dend(us_hc_avg, k = 4, k_colors = "jco", rect = T, main = "Average Linkage Cluster")
```

Cara membaca cluster pada dendrogram yaitu  sebagai berikut :

* Cluster 1 digambarkan oleh dendrogram berwarna abu-abu.
* Cluster 2 digambarkan oleh dendrogram berwarna kuning.
* Cluster 3 digambarkan oleh dendrogram berwarna merah.
* Cluster 4 digambarkan oleh dendrogram berwarna biru.

### Dendrogram Centroid Linkage

```{r, fig.height=5}
fviz_dend(us_hc_centroid, cex = 0.5, main = "Cluster Dendrogram Centroid Linkage")
```

Dendrogram yang terbentuk sangat berbeda dengan dedrogram pada model sebelumnya. Dalam centroid linkage terdapat pembentukan centroid atau pusat cluster antar 2 observasi. Cabang pada dendrogram  terdapat ketidak beraturan. 

Apabila dilihat dari nilai korelasi distancenya menunjukkan nilai yang sedikit cukup merepresentasikan data yang ada. 
```{r}
centroid_coph <- cophenetic(us_hc_centroid)
cor(centroid_coph, us_dist)
```

Berikut ini anggota cluster pada 4 cluster yang terbentuk.
```{r}
centroid_clust <- cutree(us_hc_centroid, k = 4)
table(centroid_clust)
```
Anggota cluster didominasi pada cluster 1 sebanyak 47 observasi, namun untuk cluster yang lain hanya memiliki 1 observasi saja. Berikut visualiasi cluster yang terbentuk untuk masing-masing dendrogramnya.

```{r, fig.height=5, warning=FALSE}
fviz_dend(us_hc_centroid, k = 4, k_colors = "jco", rect = T, main = "Centroid Linkage Cluster")
```

Hasil partisi cluster yang terbentuk tidak sesuai dengan yang seharusnya. Berikut deskripsi anggota cluster yang seharusnya:

* Cluster 1 terdiri dari negara New Jersey hingga California.
* Cluster 2 beranggotakan North California.
* Cluster 3 beranggotakan Alaska.
* Cluster 4 beranggotakan Vermont.

### Dendrogram Ward's Minimum Variance

```{r, fig.height=5}
fviz_dend(us_hc_ward, cex = 0.5, main = "Cluster Dendrogram Ward's Minimum Variance")
```

Dendrogram yang diperoleh dari model ward's minimum variance terlihat sangat terpartisi dengan baik. Selain itu, pada setiap observasi yang memiliki nilai similarity tinggi menunjukkan height yang rendah karena sifat dari ward's yaitu meminimumkan nilai within sum of squared(wss)-nya. 

Berdasarkan nilai korelasi distance pada model ward's ini diperoleh 0.69, yang mana dari segi nilai tidak terlalu besar, namun untuk melakukan partisi model ward's cukup bisa membuat partisi untuk menentukan banyak cluster yang terbentuk.
```{r}
ward_coph <- cophenetic(us_hc_ward)
cor(ward_coph, us_dist)
```
Berikut ini anggota cluster pada 4 cluster yang terbentuk.
```{r}
ward_clust <- cutree(us_hc_ward, k = 4)
table(ward_clust)
```
Anggota cluster yang diperoleh pada model ward's cukup mirip seperti model complete linkage. Anggota cluster yang paling sedikit yaitu pada cluster 1. 

```{r, fig.height=5, warning=FALSE}
fviz_dend(us_hc_ward, k = 4, k_colors = "jco", rect = T, main = "Ward's Minimum Variance Cluster")
```

Cara membaca cluster pada dendrogram sebagai berikut :

* Cluster 1 digambarkan oleh dendrogram berwarna biru.
* Cluster 2 digambarkan oleh dendrogram berwarna kuning.
* Cluster 3 digambarkan oleh dendrogram berwarna merah
* Cluster 4 digambarkan oleh dendrogram berwarna abu-abu.

## Cluster Validation

Dalam analisis clustering, menentukan banyak cluster terbentuk serta model yang lebih baik digunakan cukup sulit. Secara umum, untuk menentukan cluster yang baik terdapat dua pengukuran yang sering dilakukan:

1. Internal measures

Internal measures merupakan pengukuran untuk melakukan cluster validation berdasarkan informasi internal yang terdapat pada data. Metode yang biasa digunakan pada internal measure yaitu *connectivity*, *dunn index*, dan *silhouette*. Nilai connectivity berkisar antara 0-inf, dimana cluster yang baik memiliki connectivity yang kecil. Nilai silhouette dan dunn index yang diharapkan adalah yang semakin besar, maka clusternya semakin baik.

2. Stability measures

Stability measures merupakan pengukuran validation cluster yang lebih mendalam dibandingkan internal measures. Stability mengukur tingkat konsistensi dari hasil cluster yang terbentuk apabila terdapat satu variabel yang dihilangkan dalam satu waktu. Pengukuran yang dilakukan pada stability yaitu:

* *Average proportion of non-overlap (APN)*: mengukur nilai proporsi rata-rata banyak observasi pada cluster yang sama pada pembuatan cluster dengan data yang lengkap dan data yang sudah dihilangkan satu variabelnya.
* *Average distance (AD)*: jarak rata-rata antar observasi dalam cluster yang sama pada pembuatan cluster dengan data yang lengkap dan data yang sudah dihilangkan satu variabelnya.
* *Average distance between means (ADM)*: jarak rata-rata antar pusat cluster pada pembuatan cluster dengan data yang lengkap dan data yang sudah dihilangkan satu variabelnya.
* *Figure of merit (FOM)*: rata-rata varians intra cluster pada variabel yang dihapus di mana clustering dilakukan pada sisa kolom yang tidak terhapus.

> Nilai dari APN, ADM, dan FOM measure stability diatas memiliki range 0-1, di mana semakin kecil nilainya, maka akan semakin konsisten hasil clustering yang terbentuk. Sedangkan nilai AD berkisar pada range 0-inf, semakin kecil nilainya juga semakin baik hasil clusternya[^6].

Bahasan lebih jauh mengenai cluster validation dapat dibaca [disini](https://www.datanovia.com/en/lessons/cluster-validation-statistics-must-know-methods/).

Untuk melakukan validasi cluster, R menyediakan package `clValid`. Dalam melakukan validasi clusternya, fungsi yang digunakan adalah `clValid()`. Kita akan coba cek banyak cluster yang optimal digunakan berdasarkan internal measures dan stability measures menggunakan data `us_z` yang sudah dilakukan standarisasi.

Fungsi `clValid()` memiliki beberapa argumen diantaranya:

* `obj`: data yang akan digunakan dalam clustering
* `nClust`: banyak cluster yang akan di evaluasi
* `clMethods`: metode clustering yang digunakan, beberapa diantaranya yang bisa digunakan yaitu "hierarchical, "kmeans", "diana", "fanny", "som", "model", "sota", "pam", "clara", dan "agnes". Bisa menggunakan lebih dari satu metode yang akan di evaluasi.
* `validation`: pengukuran validasi cluster yang digunakan, beberapa diantaranya yaitu "internal", "stability", dan "biological". Bisa menggunakan lebih dari satu metode yang akan di evaluasi.
* `metric`: pengukuran (dis)similarity yang digunakan, beberapa yang bisa digunakan yaitu "euclidean", "correlation", dan "manhattan".
* `method`: metode hierarchical clustering yang digunakan, beberapa yang bisa digunakan yaitu "ward", "single", "complete", dan "average". 


```{r}
library(clValid)
internal <- clValid(us_z, nClust = 3:5, clMethods = "hierarchical", validation = "internal", metric = "euclidean")

summary(internal_complete)
```

Berdasarkan hasil validasi cluster menggunakan dunn index dan silhouette menunjukkan banyak cluster yang optimal terbentuk yaitu 4 cluster.

```{r}
stability <- clValid(us_z, nClust = 3:4, clMethods = "hierarchical", validation = "stability")
# Display only optimal Scores
optimalScores(stability)
```

Berdasarkan hasil validasi cluster menggunakan stability measures cluster yang terbentuk dapat terbentuk 3 atau 4 cluster. 

# Conclusion

Berdasarkan analisis cluster menggunakan hierarchical clustering, banyak sekali metode yang digunakan. 

# Reference

[^1]: Tan, P.N., Steinbach, M., Kumar, V. (2006) Introduction to Data Mining. Boston: Pearson Education.

[^2]: Fred, A.L.N. dan Leitao, J.M.N. (2000) Partitional vs Hierarchical Clustering Using a Minimum Grammar Complexity Approach, di F.J. Ferri et al. (Eds.): SSPR&SPR 2000, LNCS 1876, hal. 193-202. Berlin: Springer-Verlag

[^3]: University of Cincinnati Business Analytics. UC Business Analytics R Programming Guide, Bab [Hierarchical Clustering](https://uc-r.github.io/hc_clustering)

[^4]: Rhys, H.I. (2020) [Machine Learning with R, the tidyverse, and mlr](https://livebook.manning.com/book/machine-learning-for-mortals-mere-and-otherwise/chapter-17/). USA: Manning Publications Co.

[^5]: [(Dis)similarity Measure](https://www.datanovia.com/en/lessons/clustering-distance-measures/)

[^6]: [Cluster Validation](https://www.datanovia.com/en/lessons/choosing-the-best-clustering-algorithms/)
